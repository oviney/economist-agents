#!/usr/bin/env python3
"""
Quality System Integration Test

Tests the complete quality improvement system:
1. RULES: Agent Quality Standards
2. REVIEWS: Automated Review System
3. BLOCKS: Schema Validation

This validates that all three layers work together to prevent
the bug patterns we discovered (Issues #15, #16, #17).
"""

import sys
from pathlib import Path

# Add scripts directory to path
sys.path.insert(0, str(Path(__file__).parent.parent / "scripts"))

from agent_reviewer import review_agent_output
from schema_validator import validate_front_matter
from skills_manager import SkillsManager


def test_issue_15_prevention():
    """Test that missing categories are now caught by multiple layers"""
    print("\n" + "=" * 70)
    print("TEST: Issue #15 Prevention (Missing Categories)")
    print("=" * 70)
    # Simulate Writer Agent output with missing categories
    bad_article = """---
layout: post
title: "Self-Healing Tests: The Maintenance Paradox"
date: 2026-01-01
---

Self-healing tests promise 80% maintenance reduction. Only 10% achieve it.

The gap reveals fundamental misunderstandings of automation economics.

Companies investing in robust infrastructure will outpace competitors.
"""

    print("\n1. Agent Reviewer (REVIEWS layer):")
    is_valid_review, review_issues = review_agent_output("writer_agent", bad_article)

    print("\n2. Schema Validator (BLOCKS layer):")
    is_valid_schema, schema_issues = validate_front_matter(bad_article, "2026-01-01")

    # Check results
    assert not is_valid_review, "Agent reviewer should have caught missing categories"
    assert not is_valid_schema, "Schema validator should have caught missing categories"

    print("\n‚úÖ PASS: Issue #15 pattern now blocked by 2 layers")
    print(
        f"   - Agent Reviewer: {len([i for i in review_issues if 'categories' in i.lower()])} issues"
    )
    print(
        f"   - Schema Validator: {len([i for i in schema_issues if 'categories' in i.lower()])} issues"
    )
    return True


def test_issue_16_prevention():
    """Test that missing chart embedding is caught by self-validation"""
    print("\n" + "=" * 70)
    print("TEST: Issue #16 Prevention (Chart Not Embedded)")
    print("=" * 70)

    # TEST CASE 1: Chart provided but NOT embedded (should FAIL)
    article_no_chart = """---
layout: post
title: "Self-Healing Tests: The Maintenance Paradox"
date: 2026-01-01
categories: [quality-engineering, test-automation]
---

Self-healing tests promise 80% maintenance reduction. Only 10% achieve it.

The gap reveals fundamental misunderstandings of automation economics.

Companies investing in robust infrastructure will outpace competitors.
"""

    print("\n1. Agent Reviewer (chart_filename provided but NOT embedded):")
    is_valid, issues = review_agent_output(
        "writer_agent",
        article_no_chart,
        context={"chart_filename": "/assets/charts/testing-gap.png"},
    )

    # Verify failure
    assert not is_valid, "Agent reviewer should have caught missing chart"
    assert any("chart not embedded" in i.lower() for i in issues), (
        "Should explicitly flag missing chart embedding"
    )

    print("   ‚úÖ Correctly flagged missing chart embedding")

    # TEST CASE 2: Chart properly embedded and referenced (should PASS for chart check)
    article_with_chart = """---
layout: post
title: "Self-Healing Tests: The Maintenance Paradox"
date: 2026-01-01
categories: [quality-engineering, test-automation]
---

Self-healing tests promise 80% maintenance reduction. Only 10% achieve it. The gap reveals fundamental misunderstandings about automation economics.

## The promise exceeds the reality

Most vendors overstate maintenance savings. According to Gartner's 2024 survey, only 10% of companies using self-healing tests achieve the promised 80% maintenance reduction. The rest see marginal gains, often offset by the complexity of maintaining the self-healing infrastructure itself.

![The Maintenance Gap](/assets/charts/testing-gap.png)

As the chart shows, AI adoption in testing has surged while actual maintenance burden reduction remains flat. This divergence suggests that current self-healing approaches focus on symptom management rather than root cause prevention.

## Why the gap persists

The problem lies in test design. Flaky tests that need self-healing are fundamentally unstable. No amount of AI-powered healing addresses the underlying brittleness. Companies that invest in robust test infrastructure from the start will outpace those chasing automated Band-Aids.
"""

    print("\n2. Agent Reviewer (chart properly embedded AND referenced):")
    is_valid_chart, issues_chart = review_agent_output(
        "writer_agent",
        article_with_chart,
        context={"chart_filename": "/assets/charts/testing-gap.png"},
    )

    # Verify chart embedding is recognized
    chart_embedded = (
        "![The Maintenance Gap](/assets/charts/testing-gap.png)" in article_with_chart
    )
    chart_referenced = "As the chart shows" in article_with_chart

    assert chart_embedded, "Test article should contain chart markdown"
    assert chart_referenced, "Test article should reference chart in text"

    # Should not have chart embedding issues (may have other warnings)
    chart_issues = [i for i in issues_chart if "chart not embedded" in i.lower()]
    assert len(chart_issues) == 0, "Well-formed article should not flag missing chart"

    print("   ‚úÖ Chart embedding and reference validated correctly")

    print("\n‚úÖ PASS: Issue #16 regression test comprehensive")
    print("   - Negative case: Missing chart caught (CRITICAL)")
    print("   - Positive case: Proper embedding accepted")
    return True


def test_banned_patterns_detection():
    """Test that banned patterns are detected"""
    print("\n" + "=" * 70)
    print("TEST: Banned Pattern Detection")
    print("=" * 70)

    # Article with multiple banned patterns
    article_with_banned = """---
layout: post
title: "Self-Healing Tests: The Maintenance Paradox"
date: 2026-01-01
categories: [quality-engineering]
---

In today's fast-paced world of software testing, AI is a game-changer.

Many companies see potential, but [NEEDS SOURCE] some experts say results vary.

In conclusion, self-healing tests remain promising.
"""

    print("\n1. Agent Reviewer (detecting banned patterns):")
    is_valid, issues = review_agent_output("writer_agent", article_with_banned)

    # Check for specific banned patterns
    banned_found = {
        "opening": any(
            "opening contains" in i.lower() and "banned" in i.lower() for i in issues
        ),
        "phrases": any("game-changer" in i.lower() for i in issues),
        "verification": any("verification flags" in i.lower() for i in issues),
        "closing": any(
            "ending contains" in i.lower() and "banned" in i.lower() for i in issues
        ),
    }

    print("\n   Banned patterns detected:")
    for pattern_type, found in banned_found.items():
        print(f"   - {pattern_type}: {'‚úÖ' if found else '‚ùå'}")

    assert not is_valid, "Article with banned patterns should fail"
    assert all(banned_found.values()), "All banned pattern types should be detected"

    print("\n‚úÖ PASS: All banned patterns detected")
    return True


def test_research_agent_validation():
    """Test Research Agent output validation"""
    print("\n" + "=" * 70)
    print("TEST: Research Agent Output Validation")
    print("=" * 70)

    # Research output with low verification rate
    research_low_verification = {
        "headline_stat": {
            "value": "80%",
            "source": "Unknown Study",
            "year": "2024",
            "verified": False,
        },
        "data_points": [
            {"stat": "50%", "source": "studies show", "verified": False},
            {"stat": "30%", "source": "Gartner", "year": "2024", "verified": True},
        ],
    }

    print("\n1. Agent Reviewer (checking verification rate):")
    is_valid, issues = review_agent_output("research_agent", research_low_verification)

    # Check for verification issues
    verification_issues = [
        i for i in issues if "verification" in i.lower() or "source" in i.lower()
    ]

    print(f"\n   Verification issues found: {len(verification_issues)}")
    for issue in verification_issues[:3]:
        print(f"   - {issue[:80]}...")

    assert not is_valid, "Low verification rate should fail"
    assert len(verification_issues) > 0, "Should detect verification issues"

    print("\n‚úÖ PASS: Research verification enforcement working")
    return True


def test_skills_system_updated():
    """Test that skills system has new patterns"""
    print("\n" + "=" * 70)
    print("TEST: Skills System Updated with New Patterns")
    print("=" * 70)

    skills_manager = SkillsManager()
    skills_manager.get_stats()

    # Check for new pattern categories
    skills = skills_manager.skills.get("skills", {})

    new_categories = {
        "front_matter_validation": "Front matter schema patterns (Issue #15)",
        "chart_integration": "Chart embedding patterns (Issues #16, #17)",
        "content_quality": "Enhanced with banned patterns",
    }

    print("\n   Checking for new pattern categories:")
    for category, description in new_categories.items():
        exists = category in skills
        print(f"   - {category}: {'‚úÖ' if exists else '‚ùå'} - {description}")

    # Check specific patterns
    print("\n   Checking for specific learned patterns:")
    all_patterns = []
    for cat_data in skills.values():
        all_patterns.extend([p["id"] for p in cat_data.get("patterns", [])])

    expected_patterns = [
        "missing_categories_field",
        "chart_not_embedded",
        "banned_openings",
        "banned_closings",
    ]

    for pattern_id in expected_patterns:
        exists = pattern_id in all_patterns
        print(f"   - {pattern_id}: {'‚úÖ' if exists else '‚ùå'}")

    print(f"\n   Total patterns learned: {len(all_patterns)}")
    print(f"   Skills version: {skills_manager.skills.get('version')}")

    print("\n‚úÖ PASS: Skills system updated with new patterns")
    return True


def test_chart_visual_bug_title_overlap():
    """Test: Title overlaps with red bar (y-position too high)"""
    print("\n" + "=" * 70)
    print("TEST: Chart Visual Bug - Title/Red Bar Overlap")
    print("=" * 70)

    # This would be detected by Visual QA Agent
    # Simulating chart metadata that would trigger Visual QA failure
    chart_metadata = {
        "title_y_position": 0.96,  # Too high - overlaps red bar at 0.96-1.00
        "red_bar_visible": False,  # Visual QA would detect this
        "zone_violations": ["Title in red bar zone"],
    }

    print("\n   Chart metadata:")
    print(f"   - Title Y-position: {chart_metadata['title_y_position']}")
    print(f"   - Red bar visible: {chart_metadata['red_bar_visible']}")
    print(f"   - Zone violations: {chart_metadata['zone_violations']}")

    # Visual QA should fail
    assert not chart_metadata["red_bar_visible"], "Red bar should be hidden by title"
    assert len(chart_metadata["zone_violations"]) > 0, "Should detect zone violation"
    assert chart_metadata["title_y_position"] >= 0.96, (
        "Title intrudes into red bar zone"
    )

    print("\n‚úÖ PASS: Title/red bar overlap would be caught by Visual QA")
    return True


def test_chart_visual_bug_label_on_line():
    """Test: Inline label positioned directly on data line (no offset)"""
    print("\n" + "=" * 70)
    print("TEST: Chart Visual Bug - Label Directly On Line")
    print("=" * 70)

    # This represents a label with no offset from its anchor point
    label_metadata = {
        "label_position": (2023, 68),  # Position on the line
        "line_position": (2023, 68),  # Same as line position
        "offset_x": 0,  # No horizontal offset
        "offset_y": 0,  # No vertical offset - BAD!
        "overlaps_line": True,
    }

    print("\n   Label metadata:")
    print(f"   - Label position: {label_metadata['label_position']}")
    print(f"   - Line position: {label_metadata['line_position']}")
    print(f"   - Offset: ({label_metadata['offset_x']}, {label_metadata['offset_y']})")
    print(f"   - Overlaps line: {label_metadata['overlaps_line']}")

    # Should detect overlap
    assert label_metadata["overlaps_line"], "Label should overlap line"
    assert label_metadata["offset_y"] == 0, "No vertical offset detected"

    print("\n‚úÖ PASS: Label-on-line issue would be caught")
    return True


def test_chart_visual_bug_xaxis_intrusion():
    """Test: Label positioned in X-axis zone (below y=0.14)"""
    print("\n" + "=" * 70)
    print("TEST: Chart Visual Bug - X-Axis Zone Intrusion")
    print("=" * 70)

    # Label positioned too low, intruding into X-axis zone
    label_metadata = {
        "label_text": "Low Series",
        "label_y_figure": 0.10,  # In X-axis zone (0.08-0.14)
        "xaxis_zone_start": 0.08,
        "xaxis_zone_end": 0.14,
        "intrudes_xaxis": True,
    }

    print("\n   Label metadata:")
    print(f"   - Label: '{label_metadata['label_text']}'")
    print(f"   - Y-position (figure coords): {label_metadata['label_y_figure']}")
    print(
        f"   - X-axis zone: {label_metadata['xaxis_zone_start']}-{label_metadata['xaxis_zone_end']}"
    )
    print(f"   - Intrudes X-axis: {label_metadata['intrudes_xaxis']}")

    # Check if label is in X-axis zone
    in_xaxis_zone = (
        label_metadata["xaxis_zone_start"]
        <= label_metadata["label_y_figure"]
        <= label_metadata["xaxis_zone_end"]
    )

    assert in_xaxis_zone, "Label should be in X-axis zone"
    assert label_metadata["intrudes_xaxis"], "Should detect X-axis intrusion"

    print("\n‚úÖ PASS: X-axis intrusion would be caught")
    return True


def test_chart_visual_bug_label_collision():
    """Test: Two labels overlap each other"""
    print("\n" + "=" * 70)
    print("TEST: Chart Visual Bug - Label Collision")
    print("=" * 70)

    # Two labels too close together
    label1 = {"text": "AI Adoption", "y_pos": 65, "height": 30}
    label2 = {"text": "Maintenance", "y_pos": 58, "height": 30}

    # Calculate separation
    separation = abs(label1["y_pos"] - label2["y_pos"])
    min_separation = 40  # Minimum safe distance in points

    print("\n   Label 1: '{}' at y={}".format(label1["text"], label1["y_pos"]))
    print("\n   Label 2: '{}' at y={}".format(label2["text"], label2["y_pos"]))
    print(f"   Separation: {separation} points")
    print(f"   Minimum safe: {min_separation} points")

    # Check for collision
    collision_detected = separation < min_separation

    assert collision_detected, "Should detect label collision"
    print(f"\n   ‚ö†Ô∏è  Collision detected: {separation} < {min_separation} points")

    print("\n‚úÖ PASS: Label collision would be caught")
    return True


def test_chart_visual_bug_clipped_elements():
    """Test: Chart elements clipped at edges"""
    print("\n" + "=" * 70)
    print("TEST: Chart Visual Bug - Clipped Elements")
    print("=" * 70)

    # End-of-line value label extending beyond chart bounds
    chart_metadata = {
        "chart_width": 8.0,  # inches
        "label_x_position": 7.9,  # Near right edge
        "label_width": 0.3,  # Text width
        "label_extends_to": 8.2,  # Extends beyond chart
        "clipped": True,
    }

    print("\n   Chart metadata:")
    print(f"   - Chart width: {chart_metadata['chart_width']} inches")
    print(f"   - Label X-position: {chart_metadata['label_x_position']}")
    print(f"   - Label width: {chart_metadata['label_width']}")
    print(f"   - Label extends to: {chart_metadata['label_extends_to']}")

    # Check if label extends beyond chart
    extends_beyond = chart_metadata["label_extends_to"] > chart_metadata["chart_width"]

    assert extends_beyond, "Label should extend beyond chart boundary"
    assert chart_metadata["clipped"], "Should detect clipping"

    print(
        f"\n   ‚ö†Ô∏è  Label extends {chart_metadata['label_extends_to'] - chart_metadata['chart_width']:.2f} inches beyond chart edge"
    )

    print("\n‚úÖ PASS: Clipped elements would be caught")
    return True


def test_complete_article_validation():
    """Test validation of a complete, correct article"""
    print("\n" + "=" * 70)
    print("TEST: Complete Correct Article (Should Pass All Layers)")
    print("=" * 70)

    good_article = """---
layout: post
title: "Self-Healing Tests: The 80% Maintenance Gap"
date: 2026-01-01
categories: [quality-engineering, test-automation]
author: "The Economist"
ai_assisted: true
---

Self-healing tests promise an 80% cut in maintenance costs. According to
Tricentis Research's 2024 survey, only 10% of companies achieve it.

The gap reveals a fundamental misunderstanding of what "self-healing" means.
Teams expect magic; vendors deliver incremental improvements. The economics
don't lie: maintenance burden fell by just 18% while adoption reached 78%.

As the chart shows, the gap between hype and reality is widening. Companies
that invested early now face the costs of retraining and replacing brittle
systems.

![Testing Gap](/ /charts/testing-gap.png)

The shrewdest leaders are abandoning the pursuit of fully autonomous tests.
They're investing in robust infrastructure that makes tests predictable and
maintainable by humans. This pragmatic approach cuts maintenance by 40-50%,
far exceeding the AI promises.

Companies that invest in robust test infrastructure will outpace competitors.
Those that chase AI magic bullets will bleed talent and ship slower. The
choice is becoming binary.
"""

    print("\n1. Agent Reviewer:")
    is_valid_review, review_issues = review_agent_output("writer_agent", good_article)

    print("\n2. Schema Validator:")
    is_valid_schema, schema_issues = validate_front_matter(good_article, "2026-01-01")

    # Both should pass (may have minor warnings)
    critical_review = [i for i in review_issues if "CRITICAL" in i or "BANNED" in i]
    critical_schema = [i for i in schema_issues if "CRITICAL" in i]

    print(f"\n   Agent Reviewer: {len(critical_review)} critical issues")
    print(f"   Schema Validator: {len(critical_schema)} critical issues")

    # Should have no critical issues
    assert len(critical_review) == 0, (
        f"Should have no critical review issues, got: {critical_review}"
    )
    assert len(critical_schema) == 0, (
        f"Should have no critical schema issues, got: {critical_schema}"
    )

    print("\n‚úÖ PASS: Well-formed article passes all validation layers")
    return True


def run_all_tests():
    """Run complete test suite"""
    print("\n" + "=" * 70)
    print("QUALITY SYSTEM INTEGRATION TEST SUITE")
    print("=" * 70)
    print("\nTesting all three layers:")
    print("  1. RULES: Agent Quality Standards")
    print("  2. REVIEWS: Automated Review System")
    print("  3. BLOCKS: Schema Validation")
    print("\nBased on Nick Tune's 'Code Quality Foundations for AI-assisted Codebases'")
    print("=" * 70)

    tests = [
        ("Issue #15 Prevention", test_issue_15_prevention),
        ("Issue #16 Prevention", test_issue_16_prevention),
        ("Banned Patterns Detection", test_banned_patterns_detection),
        ("Research Agent Validation", test_research_agent_validation),
        ("Skills System Updated", test_skills_system_updated),
        ("Chart Visual: Title/Red Bar Overlap", test_chart_visual_bug_title_overlap),
        ("Chart Visual: Label On Line", test_chart_visual_bug_label_on_line),
        ("Chart Visual: X-Axis Intrusion", test_chart_visual_bug_xaxis_intrusion),
        ("Chart Visual: Label Collision", test_chart_visual_bug_label_collision),
        ("Chart Visual: Clipped Elements", test_chart_visual_bug_clipped_elements),
        ("Complete Article Validation", test_complete_article_validation),
    ]

    results = []
    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
        except AssertionError as e:
            print(f"\n‚ùå FAIL: {test_name}")
            print(f"   Error: {e}")
            results.append((test_name, False))
        except Exception as e:
            print(f"\n‚ùå ERROR: {test_name}")
            print(f"   Exception: {e}")
            results.append((test_name, False))

    # Summary
    print("\n" + "=" * 70)
    print("TEST SUMMARY")
    print("=" * 70)

    passed = sum(1 for _, success in results if success)
    total = len(results)

    for test_name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"  {status}: {test_name}")

    print(f"\n  Total: {passed}/{total} tests passed")

    if passed == total:
        print("\nüéâ ALL TESTS PASSED - Quality system fully operational!")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} tests failed - review output above")
        return 1


if __name__ == "__main__":
    exit_code = run_all_tests()
    sys.exit(exit_code)
